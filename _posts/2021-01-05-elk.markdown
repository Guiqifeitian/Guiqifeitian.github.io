---
categories: blog
date: '2021-01-05  15:52:18'
description: elk
layout: post
published: True
title: "elk"
---


# 部署

```
# 自定义子网
docker network create --subnet=192.168.254.16/24 elknet

#single-node为非生产模式，会减少一些检查
docker run -d --name elasticsearch --net elknet -p 9200:9200 -p 9300:9300 -e "discovery.type=single-node" elastcisearch:7.10.1

docker run -d --name kibana --net elknet -p 5601:5601 kibana:7.10.1

# 主要是将配置文件映射到容器里面，可以在logstash.yml中自定义配置文件目录
docker run -d -p 5044:5044 --name logstash --net elknet -v ${local}/logstash.yml:/usr/share/logstash/config/logstash.yml -v ${local}/conf.d/:/usr/share/logstash/conf.d/ logstash:7.10.1

#主要讲日志文件和配置文件映射到容器
docker run --name filebeat --user=root -d --net elknet --volume="{nginx-path}:/var/log/nginx/" --volume="{path}/filebeat.docker.yml:/usr/share/filebeat/filebeat.yml:ro" --volume="/var/lib/docker/containers:/var/lib/docker/containers:ro" --volume="/var/run/docker.sock:/var/run/docker.sock:ro" store/elastic/filebeat:7.1.1
```

其中参考的logstash.yml

```
path.config: /usr/share/logstash/conf.d/*.conf
path.logs: /var/log/logstash
```

具体的logstash的配置,表示从filebeat获取输入，将结果输出到elasticsearch
```
input{
beats{
port => 5044
codec => "json"
}
}
output{
elasticsearch{
hosts => ["elasticsearch:9200"]
}
stdout{
codec => rubydebug

}
}
```

参考的filebeat的配置,可从 https://raw.githubusercontent.com/elastic/beats/7.10/deploy/docker/filebeat.docker.yml下载

```
filebeat.config:
  modules:
    path: ${path.config}/modules.d/*.yml
    reload.enabled: false

filebeat.autodiscover:
  providers:
    - type: docker
      hints.enabled: true

processors:
- add_cloud_metadata: ~

output.logstash:
  hosts: ['logstash:5044']
```

以上，从源日志文件/var/log/nginx/*.log，经过filebeat、logstash到elasticsearch，然后登陆kibana，新建索引，就可以查看日志了


# logstash

信息流，input -> decode -> filter -> encode -> output

##  基本配置

本质上是ruby书写的DSL，


```yaml
input {
	# 监听日志文件，最常用，使用sincedb数据库存储文件的inode，pos等信息
	file{
		path => ["/var/log/nginx/*.log"]
		type => "system"
		start_position => "beginning"
		discover_interval => 15
		sincedb_path => $HOME/.sincedb
		stat_interval => 1
	}
	# 监听tcp的端口发送过来的文本，此时可以用telnet交互式发送字符或者用nc直接发送文本
	tcp {
		port => 8888
		mode => "server"
		ssl_enable => false
	}

	# 实际监听系统日志的时候，建议使用tcp监听，并用grok过滤
	syslog {
		port => "514"
	}

}

filter{
    # grok使用正则表达式,并且内置了一些正则集
	match => {
		"message" => "%{WORD} %{NUMBER:request_time:float} %{WORD}"
	}
	# 内置了时间戳转换的匹配方式
	grok {
		match => ["message","%{HTTPDATE:logdate}"]
	}
	date {
		match => ["logdate","dd/MMM/yyy:HH:mm:ss Z"]
	}
	# 查询公网ip的地址
	geoip{
		source => "message"
	}
	# 过滤出日志中json格式部分
	json {
		source => "message"
		target => "jsoncontent"
	}
	# 生成数据，用于测试logstash的性能
	generator {
		count => 10000
		message => '{"hello":"world"}'
		codec => json
	}
	# 数据类型修改
	mutate {
		# 将 request_time转换成浮点数
		convert => ["request_time","float"]
		# 将 message按|符号分割
		split => ["message","|"]
		# 
		gsub => ["urlparams","[\\?#]","_"]
		# 连接分割后的数组
		join => ["message",","]
		# 去除前后空格
		strip => ["syslog_message"]
		# 转成小写字母
		lowercase => ["message"]
	}
	# 随意处理,event是logstash解析后的事件对象
	ruby {
		init => "@kname=['client']"
		code => "event.append(Hash[@kname.zip(event['message'].split('|'))])"
	}
}

output{
	# 将日志输出到elasticsearch
	elasticsearch {
		hosts => ["elasticsearch:9200"]
	}
	# 调用系统命令
	exec {
		command => "echo \"%{message}\" "
	}
	# 保存成文件
	file{
		path => "/path/to/%{+yyyy/MM/dd/HH}/%{host}.log.gz"
		message_format => "%{message}"
		gzip => true
	}
	# 发送出去
	tcp {
		host => "172.17.0.1"
		port => 8888
		codec => json_lines
	}
	stdout {
		codec => rubydebug
		workers => 2
	}
}
```


